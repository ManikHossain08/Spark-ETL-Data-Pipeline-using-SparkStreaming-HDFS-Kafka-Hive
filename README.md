# Spark-ETL-Data-Pipeline-using-SparkStreaming-HDFS-Kafka-Hive

# OBJECTIVE
The objectives of this project are to get experience of coding with:
  • Spark
  • Spark SQL
  • Spark Streaming
  • Kafka
  • Scala and functional programming

# DATA SET
The data set is the one that you analyzed in Course 1 and it is STM GTFS data.

# PROBLEM STATEMENT
We get the information of STM every day and need to run an ETL pipeline to enrich data for reporting and
analysis purpose in real-time. Data is split in two
  1. A set of tables that build dimension (batch style)
  2. Stop times that needed to be enriched for analysis and reporting (streaming)

<img width="869" alt="image" src="https://user-images.githubusercontent.com/45977153/115934549-41ef9200-a45f-11eb-8a82-4cef27396fc5.png">
<img width="869" alt="image" src="https://user-images.githubusercontent.com/45977153/115934568-4e73ea80-a45f-11eb-95a9-8be94dc3f054.png">
